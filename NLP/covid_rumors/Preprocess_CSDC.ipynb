{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27af4013-95b1-427a-a5e0-98d599b16e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import listdir\n",
    "from os import walk\n",
    "from os.path import isfile, join\n",
    "import datetime as dt\n",
    "\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9912577b-056f-4cbf-9f17-f64fd9596880",
   "metadata": {},
   "outputs": [],
   "source": [
    "rumor_content_dir = \"./data/rumor/rumor_weibo\"\n",
    "rumor_comment_dir = \"./data/rumor/rumor_forward_comment\"\n",
    "\n",
    "content_files = [f for f in listdir(rumor_content_dir) if isfile(join(rumor_content_dir, f)) and f.endswith('.json')]\n",
    "comment_files = [f for f in listdir(rumor_comment_dir) if isfile(join(rumor_comment_dir, f)) and f.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9e9b46-33b0-4aba-b3c0-57a0d688b41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(324, 266)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_files), len(comment_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0db468-aa4f-42be-ab90-3ff4b7124547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dataframe to store preprocessed information\n",
    "ids = [i for i in range(max(len(content_files), len(comment_files)))]\n",
    "\n",
    "rumor_data = pd.DataFrame({'id': ids})\n",
    "data_num = len(rumor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fe8508-e9ca-43a5-963f-1e7d8bceb609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_file_name(file_name):\n",
    "    date_from_name = file_name.split('_')[0]\n",
    "    return dt.datetime.strptime(date_from_name,'%Y-%m-%d').date()\n",
    "\n",
    "def md_to_text(md):\n",
    "    \"\"\"\n",
    "    Inspired by: https://stackoverflow.com/questions/761824/python-how-to-convert-markdown-formatted-text-to-text\n",
    "    \"\"\"\n",
    "    html = markdown.markdown(md)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def cleanup_text(text):\n",
    "    # remove space\n",
    "    text = map(lambda s: s.replace(' ', ''), text)\n",
    "    return ''.join(list(text))\n",
    "\n",
    "def remove_user_name(text):\n",
    "    if '：' not in text:\n",
    "        return text\n",
    "    split_list = text.split('：')\n",
    "    final_text = ''\n",
    "    for i in range(1, len(split_list)):\n",
    "        final_text += split_list[i]\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38a96ec1-e66d-4096-aaed-9691a6be6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-22\n",
      "2020-01-22\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-23\n",
      "2020-01-24\n",
      "2020-01-24\n",
      "2020-01-24\n",
      "2020-01-24\n",
      "2020-01-24\n",
      "2020-01-24\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-25\n",
      "2020-01-26\n",
      "2020-01-26\n",
      "2020-01-26\n",
      "2020-01-26\n",
      "2020-01-27\n",
      "2020-01-27\n",
      "2020-01-27\n",
      "2020-01-28\n",
      "2020-01-28\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-29\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-30\n",
      "2020-01-31\n",
      "2020-01-31\n",
      "2020-01-31\n",
      "2020-02-01\n",
      "2020-02-01\n",
      "2020-02-01\n",
      "2020-02-01\n",
      "2020-02-02\n",
      "2020-02-02\n",
      "2020-02-02\n",
      "2020-02-02\n",
      "2020-02-02\n",
      "2020-02-02\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-03\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-04\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-05\n",
      "2020-02-06\n",
      "2020-02-06\n",
      "2020-02-06\n",
      "2020-02-06\n",
      "2020-02-06\n",
      "2020-02-06\n",
      "2020-02-06\n",
      "2020-02-06\n",
      "2020-02-07\n",
      "2020-02-07\n",
      "2020-02-07\n",
      "2020-02-07\n",
      "2020-02-07\n",
      "2020-02-07\n",
      "2020-02-07\n",
      "2020-02-07\n",
      "2020-02-07\n",
      "2020-02-08\n",
      "2020-02-08\n",
      "2020-02-08\n",
      "2020-02-08\n",
      "2020-02-09\n",
      "2020-02-10\n",
      "2020-02-10\n",
      "2020-02-11\n",
      "2020-02-12\n",
      "2020-02-12\n",
      "2020-02-12\n",
      "2020-02-12\n",
      "2020-02-12\n",
      "2020-02-13\n",
      "2020-02-14\n",
      "2020-02-15\n",
      "2020-02-16\n",
      "2020-02-16\n",
      "2020-02-16\n",
      "2020-02-16\n",
      "2020-02-16\n",
      "2020-02-16\n",
      "2020-02-17\n",
      "2020-02-17\n",
      "2020-02-17\n",
      "2020-02-17\n",
      "2020-02-18\n",
      "2020-02-18\n",
      "2020-02-18\n",
      "2020-02-18\n",
      "2020-02-18\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-19\n",
      "2020-02-20\n",
      "2020-02-20\n",
      "2020-02-20\n",
      "2020-02-20\n",
      "2020-02-20\n",
      "2020-02-20\n",
      "2020-02-20\n",
      "2020-02-20\n",
      "2020-02-21\n",
      "2020-02-21\n",
      "2020-02-21\n",
      "2020-02-21\n",
      "2020-02-21\n",
      "2020-02-22\n",
      "2020-02-22\n",
      "2020-02-22\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-23\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-24\n",
      "2020-02-25\n",
      "2020-02-25\n",
      "2020-02-25\n",
      "2020-02-25\n",
      "2020-02-25\n",
      "2020-02-25\n",
      "2020-02-26\n",
      "2020-02-26\n",
      "2020-02-26\n",
      "2020-02-26\n",
      "2020-02-26\n",
      "2020-02-26\n",
      "2020-02-26\n",
      "2020-02-27\n",
      "2020-02-27\n",
      "2020-02-27\n",
      "2020-02-28\n",
      "2020-02-29\n",
      "2020-02-29\n",
      "2020-02-29\n",
      "2020-02-29\n",
      "2020-02-29\n",
      "2020-02-29\n",
      "2020-02-29\n",
      "2020-02-29\n",
      "2020-02-29\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-02\n",
      "2020-03-02\n",
      "2020-03-02\n",
      "2020-03-02\n",
      "2020-03-02\n",
      "2020-03-02\n",
      "2020-03-02\n",
      "2020-03-02\n",
      "2020-03-03\n",
      "2020-03-03\n",
      "2020-03-03\n",
      "2020-03-03\n",
      "2020-03-03\n"
     ]
    }
   ],
   "source": [
    "# extract rumor file name, rumor content, visit times, and data from the data\n",
    "\n",
    "rumor_names = [np.nan] * data_num\n",
    "rumor_dates = [np.nan] * data_num\n",
    "contents = [np.nan] * data_num\n",
    "visit_times = [np.nan] * data_num\n",
    "\n",
    "for i, content_file in enumerate(content_files):\n",
    "\n",
    "    rumor_names[i] = content_file\n",
    "    rumor_dates[i] = get_date_from_file_name(content_file)\n",
    "    print(rumor_dates[i])\n",
    "\n",
    "    file_path = join(rumor_content_dir, content_file)\n",
    "\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "        if data['rumorText']:\n",
    "            contents[i] = cleanup_text(md_to_text(data['rumorText']))\n",
    "        if data['visitTimes']:\n",
    "            visit_times[i] = data['visitTimes']\n",
    "\n",
    "rumor_data['name'] = rumor_names\n",
    "rumor_data['date'] = rumor_dates\n",
    "rumor_data['content'] = contents\n",
    "rumor_data['visit_times'] = visit_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f3ada89-2e56-4274-a23c-343b69de21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract comment count, forward count, texts(a list)\n",
    "\n",
    "comment_counts = [np.nan] * data_num\n",
    "forward_counts = [np.nan] * data_num\n",
    "texts = [np.nan] * data_num\n",
    "\n",
    "for comment_file in comment_files:\n",
    "    if comment_file not in content_files:\n",
    "        continue\n",
    "        \n",
    "    id = content_files.index(comment_file)\n",
    "\n",
    "    file_path = join(rumor_comment_dir, comment_file)\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    action_texts = []\n",
    "    comment_count = 0\n",
    "    forward_count = 0\n",
    "    # step through the information\n",
    "    for info in data:\n",
    "        if info['comment_or_forward'] == 'forward':\n",
    "            forward_count += 1\n",
    "        elif info['comment_or_forward'] == 'comment':\n",
    "            comment_count += 1\n",
    "        if info['text']:\n",
    "            content = remove_user_name(md_to_text(info['text']))\n",
    "            action_texts.append(cleanup_text(content))\n",
    "\n",
    "    # assign to corresponds global list\n",
    "    comment_counts[id] = comment_count\n",
    "    forward_counts[id] = forward_count\n",
    "    texts[id] = action_texts\n",
    "\n",
    "# append to dataframe\n",
    "rumor_data['comment_times'] = comment_counts\n",
    "rumor_data['forward_times'] = forward_counts\n",
    "rumor_data['action_texts'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f13add-28d6-40d3-b2a2-4b54c840f7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>visit_times</th>\n",
       "      <th>comment_times</th>\n",
       "      <th>forward_times</th>\n",
       "      <th>action_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-22_K1CaS7Qth660h.json</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-22_K1CaS7Qxd76ol.json</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺—-一种犬科动物携带的病毒，然...</td>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[转发微博, @中国政府网]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-23_K1CaS7Q1c768i.json</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺—-一种犬科动物携带的病毒，然...</td>\n",
       "      <td>71</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[根据什么，？？中央都没确定，你就确定研究了？？？, @微博辟谣已举报, 央视新闻已辟谣¡评...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-23_K1CaS7Qth7qgi.json</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>今天下午五点~九点，大家千万不要出门，全市转运发热病人到定点医院。切记切记。以免造成感染。中...</td>\n",
       "      <td>26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[不会吧, 空军播撒消毒液，第一次听说这种操作, 谣言，请删除, 已经撒了，闻到了]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-23_K1CaS7Qth7qsf.json</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>说是今天要在武汉上空撒消毒粉液之后还要运转发热患者到定点医院。​​​​</td>\n",
       "      <td>25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[官方辟谣了, 消毒那个辟谣吗, 确定吗？, 辟谣了]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-23_K1CaS7Qth7qsh.json</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>武汉“封城“隔离后并不是什么都不做，现在开始空中撒消毒粉液全城消毒了。老百姓更在意的是实实在...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                           name        date  \\\n",
       "0   0  2020-01-22_K1CaS7Qth660h.json  2020-01-22   \n",
       "1   1  2020-01-22_K1CaS7Qxd76ol.json  2020-01-22   \n",
       "2   2  2020-01-23_K1CaS7Q1c768i.json  2020-01-23   \n",
       "3   3  2020-01-23_K1CaS7Qth7qgi.json  2020-01-23   \n",
       "4   4  2020-01-23_K1CaS7Qth7qsf.json  2020-01-23   \n",
       "5   5  2020-01-23_K1CaS7Qth7qsh.json  2020-01-23   \n",
       "\n",
       "                                             content  visit_times  \\\n",
       "0                                                NaN           39   \n",
       "1  据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺—-一种犬科动物携带的病毒，然...           33   \n",
       "2  据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺—-一种犬科动物携带的病毒，然...           71   \n",
       "3  今天下午五点~九点，大家千万不要出门，全市转运发热病人到定点医院。切记切记。以免造成感染。中...           26   \n",
       "4                说是今天要在武汉上空撒消毒粉液之后还要运转发热患者到定点医院。​​​​           25   \n",
       "5  武汉“封城“隔离后并不是什么都不做，现在开始空中撒消毒粉液全城消毒了。老百姓更在意的是实实在...           57   \n",
       "\n",
       "   comment_times  forward_times  \\\n",
       "0            NaN            NaN   \n",
       "1            1.0            1.0   \n",
       "2           18.0            0.0   \n",
       "3            4.0            0.0   \n",
       "4            4.0            0.0   \n",
       "5            0.0            0.0   \n",
       "\n",
       "                                        action_texts  \n",
       "0                                                NaN  \n",
       "1                                     [转发微博, @中国政府网]  \n",
       "2  [根据什么，？？中央都没确定，你就确定研究了？？？, @微博辟谣已举报, 央视新闻已辟谣¡评...  \n",
       "3         [不会吧, 空军播撒消毒液，第一次听说这种操作, 谣言，请删除, 已经撒了，闻到了]  \n",
       "4                        [官方辟谣了, 消毒那个辟谣吗, 确定吗？, 辟谣了]  \n",
       "5                                                 []  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rumor_data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0756d885-3598-4f4b-b43e-efa0b6be1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out the raw data for future reference\n",
    "rumor_data.to_csv('./data/rumor/analys_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6854af0b-d365-4813-b404-0bd7b86e07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Testing ######\n",
    "full_data = pd.read_csv('./data/rumor/analys_data.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e27208f-5c97-4771-a445-4fd59e383a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>visit_times</th>\n",
       "      <th>comment_times</th>\n",
       "      <th>forward_times</th>\n",
       "      <th>action_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-22_K1CaS7Qth660h.json</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-22_K1CaS7Qxd76ol.json</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺—-一种犬科动物携带的病毒，然...</td>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['转发微博', '@中国政府网']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-23_K1CaS7Q1c768i.json</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺—-一种犬科动物携带的病毒，然...</td>\n",
       "      <td>71</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['根据什么，？？中央都没确定，你就确定研究了？？？', '@微博辟谣已举报', '央视新闻...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-23_K1CaS7Qth7qgi.json</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>今天下午五点~九点，大家千万不要出门，全市转运发热病人到定点医院。切记切记。以免造成感染。中...</td>\n",
       "      <td>26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['不会吧', '空军播撒消毒液，第一次听说这种操作', '谣言，请删除', '已经撒了，闻...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-23_K1CaS7Qth7qsf.json</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>说是今天要在武汉上空撒消毒粉液之后还要运转发热患者到定点医院。​​​​</td>\n",
       "      <td>25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['官方辟谣了', '消毒那个辟谣吗', '确定吗？', '辟谣了']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>2020-03-03_K1CaS8wtj7aoi.json</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>\"我不是中国人\"，华人女子获美国绿卡，慷慨对美捐赠20万只口罩\"我已经获得美国绿卡，即将加入...</td>\n",
       "      <td>108</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['转发微博', '//@syll天天天蓝:长大不要娘的人还是少数', '有钱任性', '转...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>320</td>\n",
       "      <td>320</td>\n",
       "      <td>2020-03-03_K1CaS8wtk6Kwe.json</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>【重大消息】上海市政府会议精神，3月16日居民出行正常化，17日公交正常化，18日逐步企业生...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>2020-03-03_K1CaS8wtl7qcf.json</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>测试3月4005​​​​</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>322</td>\n",
       "      <td>322</td>\n",
       "      <td>2020-03-03_K1CaS8wxc7asi.json</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>测试3月4014​​​​</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>2020-03-03_K1CaS8wxh6agf.json</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>全国解禁时间表广东：2月25日结束，已结束！广西：3月5日结束，倒计时5天！海南：2月23日...</td>\n",
       "      <td>53</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['要真是这样那真的太高兴了', '别特么造谣了']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   id                           name        date  \\\n",
       "0             0    0  2020-01-22_K1CaS7Qth660h.json  2020-01-22   \n",
       "1             1    1  2020-01-22_K1CaS7Qxd76ol.json  2020-01-22   \n",
       "2             2    2  2020-01-23_K1CaS7Q1c768i.json  2020-01-23   \n",
       "3             3    3  2020-01-23_K1CaS7Qth7qgi.json  2020-01-23   \n",
       "4             4    4  2020-01-23_K1CaS7Qth7qsf.json  2020-01-23   \n",
       "..          ...  ...                            ...         ...   \n",
       "319         319  319  2020-03-03_K1CaS8wtj7aoi.json  2020-03-03   \n",
       "320         320  320  2020-03-03_K1CaS8wtk6Kwe.json  2020-03-03   \n",
       "321         321  321  2020-03-03_K1CaS8wtl7qcf.json  2020-03-03   \n",
       "322         322  322  2020-03-03_K1CaS8wxc7asi.json  2020-03-03   \n",
       "323         323  323  2020-03-03_K1CaS8wxh6agf.json  2020-03-03   \n",
       "\n",
       "                                               content  visit_times  \\\n",
       "0                                                  NaN           39   \n",
       "1    据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺—-一种犬科动物携带的病毒，然...           33   \n",
       "2    据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺—-一种犬科动物携带的病毒，然...           71   \n",
       "3    今天下午五点~九点，大家千万不要出门，全市转运发热病人到定点医院。切记切记。以免造成感染。中...           26   \n",
       "4                  说是今天要在武汉上空撒消毒粉液之后还要运转发热患者到定点医院。​​​​           25   \n",
       "..                                                 ...          ...   \n",
       "319  \"我不是中国人\"，华人女子获美国绿卡，慷慨对美捐赠20万只口罩\"我已经获得美国绿卡，即将加入...          108   \n",
       "320  【重大消息】上海市政府会议精神，3月16日居民出行正常化，17日公交正常化，18日逐步企业生...           30   \n",
       "321                                       测试3月4005​​​​           50   \n",
       "322                                       测试3月4014​​​​           29   \n",
       "323  全国解禁时间表广东：2月25日结束，已结束！广西：3月5日结束，倒计时5天！海南：2月23日...           53   \n",
       "\n",
       "     comment_times  forward_times  \\\n",
       "0              NaN            NaN   \n",
       "1              1.0            1.0   \n",
       "2             18.0            0.0   \n",
       "3              4.0            0.0   \n",
       "4              4.0            0.0   \n",
       "..             ...            ...   \n",
       "319           22.0            4.0   \n",
       "320            0.0            0.0   \n",
       "321            0.0            0.0   \n",
       "322            0.0            0.0   \n",
       "323            2.0            0.0   \n",
       "\n",
       "                                          action_texts  \n",
       "0                                                  NaN  \n",
       "1                                   ['转发微博', '@中国政府网']  \n",
       "2    ['根据什么，？？中央都没确定，你就确定研究了？？？', '@微博辟谣已举报', '央视新闻...  \n",
       "3    ['不会吧', '空军播撒消毒液，第一次听说这种操作', '谣言，请删除', '已经撒了，闻...  \n",
       "4                  ['官方辟谣了', '消毒那个辟谣吗', '确定吗？', '辟谣了']  \n",
       "..                                                 ...  \n",
       "319  ['转发微博', '//@syll天天天蓝:长大不要娘的人还是少数', '有钱任性', '转...  \n",
       "320                                                 []  \n",
       "321                                                 []  \n",
       "322                                                 []  \n",
       "323                         ['要真是这样那真的太高兴了', '别特么造谣了']  \n",
       "\n",
       "[324 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43be6e1-0160-41ac-8453-e76ce3a0ad66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m start_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m end_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020-03-03\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m x_index \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mdate_range(start_date, end_date)\n\u001b[0;32m      5\u001b[0m x_index\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "start_date = '2020-01-01'\n",
    "end_date = '2020-03-03'\n",
    "x_index = pd.date_range(start_date, end_date)\n",
    "\n",
    "x_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bac7e-a7a1-44ca-ba7e-3ce0c2ff9fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
